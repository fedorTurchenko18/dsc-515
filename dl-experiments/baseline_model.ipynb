{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment (default operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, json, matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from data_loader.hgf_export import HGFresource\n",
    "from model_accelerator.custom_loop import CustomModel\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HGF_TOKEN = os.environ['HUGGINGFACE_TOKEN']\n",
    "HGF_DATA_REPO = os.environ['HUGGINGFACE_DATASET_V2_REPO']\n",
    "HGF_BASELINE_MODEL_REPO = os.environ['HUGGINGFACE_BASELINE_CNN_REPO']\n",
    "HGF_TOPIC_MODEL_TOP2VEC_REPO = os.environ['HUGGINGFACE_TOPIC_MODEL_TOP2VEC_REPO']\n",
    "HGF_TOPIC_MODEL_TOP2VEC_FILE = os.environ['HUGGINGFACE_TOPIC_MODEL_TOP2VEC_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgf = HGFresource(token=HGF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = hgf.load_model(repo=HGF_TOPIC_MODEL_TOP2VEC_REPO, filename=HGF_TOPIC_MODEL_TOP2VEC_FILE)\n",
    "N_CLASSES = topic_model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = hgf.load_data_tfds(repo=HGF_DATA_REPO, n_classes=N_CLASSES, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate may require fine-tuning but use the optimizer specified below\n",
    "OPTIMIZER = tf.keras.optimizers.legacy.Adam(learning_rate=0.001) # use this if you have M1/M2 Mac, otherwise use the next line\n",
    "# OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = [tf.keras.metrics.F1Score('weighted')]\n",
    "EPOCHS = 20\n",
    "INPUT_SHAPE = (256, 219, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    train_f1_scores = history.history['f1_score']\n",
    "    val_f1_scores = history.history['val_f1_score']\n",
    "\n",
    "    epochs = range(1, len(train_f1_scores) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_f1_scores, 'bo-', label='Training F1 Score')\n",
    "    plt.plot(epochs, val_f1_scores, 'ro-', label='Validation F1 Score')\n",
    "    plt.title('Training and Validation F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Code Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(256, 219, 3))\n",
    "conv1 = keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "maxpool1 = keras.layers.MaxPool2D(pool_size=(2, 2), padding='same')(conv1)\n",
    "conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu')(maxpool1)\n",
    "maxpool2 = keras.layers.MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu')(maxpool2)\n",
    "maxpool3 = keras.layers.MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "flat = keras.layers.Flatten()(maxpool3)\n",
    "dense = keras.layers.Dense(128, activation='relu')(flat)\n",
    "outputs = keras.layers.Dense(N_CLASSES, activation='softmax')(dense)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    OPTIMIZER,\n",
    "    LOSS,\n",
    "    METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Your Results to Hugging Face Once Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: when naming your weights file\n",
    "# indicate the type of model\n",
    "# e.g.:\n",
    "## 'tuned_optimizer_model_weights.h5'\n",
    "## 'data_augmentation_model_weights.h5'\n",
    "WEIGHTS_TO_COMMIT = 'baseline_model_weights.h5'\n",
    "# save model weights\n",
    "model.save_weights(WEIGHTS_TO_COMMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model config\n",
    "CONFIG_TO_COMMIT  = 'model_config.json'\n",
    "model_config = model.get_config()\n",
    "model_config = json.dumps(model_config)\n",
    "with open(CONFIG_TO_COMMIT, 'w') as f:\n",
    "    f.write(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORMANCE_TO_COMMIT = 'baseline_model_performance.json'\n",
    "performance_dict = {\n",
    "    'train': {\n",
    "        'weighted_f1': history.history['f1_score'][-1]\n",
    "    },\n",
    "    'test': {\n",
    "        'weighted_f1': history.history['val_f1_score'][-1]\n",
    "    }\n",
    "}\n",
    "performance_dict = json.dumps(performance_dict)\n",
    "with open(PERFORMANCE_TO_COMMIT, 'w') as f:\n",
    "    f.write(performance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/fedorturchenko/.cache/huggingface/token\n",
      "Login successful\n",
      "Successfully logged out.\n"
     ]
    }
   ],
   "source": [
    "hgf.commit_to_hub(\n",
    "    repo=HGF_BASELINE_MODEL_REPO,\n",
    "    path_on_local=[WEIGHTS_TO_COMMIT, CONFIG_TO_COMMIT, PERFORMANCE_TO_COMMIT],\n",
    "    path_in_repo=[WEIGHTS_TO_COMMIT, CONFIG_TO_COMMIT, PERFORMANCE_TO_COMMIT],\n",
    "    # do not forget to change your commit message\n",
    "    # to add even more clarity\n",
    "    commit_message='upload baseline model weights, config, and scores'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files from local = clean up\n",
    "for path in [WEIGHTS_TO_COMMIT, CONFIG_TO_COMMIT, PERFORMANCE_TO_COMMIT]:\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGF_BASELINE_MODEL_CONFIG = os.environ['HUGGINGFACE_BASELINE_CNN_CONFIG_FILE']\n",
    "HGF_BASELINE_MODEL_WEIGHTS = os.environ['HUGGINGFACE_BASELINE_CNN_WEIGHTS_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config of baseline model which we need to optimize\n",
    "model_config_file = hgf.load_file(repo=HGF_BASELINE_MODEL_REPO, filename=HGF_BASELINE_MODEL_CONFIG)\n",
    "with open(model_config_file, 'r') as f:\n",
    "    model_config = f.read()\n",
    "model_config = json.loads(model_config)\n",
    "# extract number of classes from the last layer\n",
    "N_CLASSES = model_config['layers'][-1]['config']['units']\n",
    "\n",
    "# alternatively, you can load the full baseline model with pre-defined weights\n",
    "# you may want to do this for the sake of performance comparison\n",
    "inputs, outputs, weights = hgf.load_model(\n",
    "    repo=HGF_BASELINE_MODEL_REPO,\n",
    "    filename={\n",
    "        'model_weights': HGF_BASELINE_MODEL_WEIGHTS,\n",
    "        'model_config': HGF_BASELINE_MODEL_CONFIG\n",
    "    },\n",
    "    model_type='func'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:CustomModel model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"custom_model\" was not an Input tensor, it was generated by layer \"input_1\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 256, 219, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n",
      "Model: \"custom_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        multiple                     0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 254, 217, 16)         448       ['input_1[1][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 127, 109, 16)         0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 125, 107, 32)         4640      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 62, 53, 32)           0         ['conv2d_1[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 60, 51, 64)           18496     ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 30, 25, 64)           0         ['conv2d_2[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 48000)                0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  6144128   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 57)                   7353      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6175065 (23.56 MB)\n",
      "Trainable params: 6175065 (23.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize the baseline model architecture\n",
    "model = CustomModel(inputs=inputs, outputs=outputs)\n",
    "# load weights if needed\n",
    "model.load_weights(weights)\n",
    "# view the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
