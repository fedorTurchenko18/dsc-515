{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment (default operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, json, matplotlib.pyplot as plt\n",
    "from data_loader.hgf_export import HGFresource\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HGF_TOKEN = os.environ['HUGGINGFACE_TOKEN']\n",
    "HGF_DATA_REPO = os.environ['HUGGINGFACE_DATASET_V2_REPO']\n",
    "HGF_BASELINE_MODEL_REPO = os.environ['HUGGINGFACE_BASELINE_CNN_REPO']\n",
    "HGF_TOPIC_MODEL_TOP2VEC_REPO = os.environ['HUGGINGFACE_TOPIC_MODEL_TOP2VEC_REPO']\n",
    "HGF_TOPIC_MODEL_TOP2VEC_FILE = os.environ['HUGGINGFACE_TOPIC_MODEL_TOP2VEC_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgf = HGFresource(token=HGF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = hgf.load_model(repo=HGF_TOPIC_MODEL_TOP2VEC_REPO, filename=HGF_TOPIC_MODEL_TOP2VEC_FILE)\n",
    "N_CLASSES = topic_model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = hgf.load_data_tfds(repo=HGF_DATA_REPO, n_classes=N_CLASSES, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate may require fine-tuning but use the optimizer specified below\n",
    "OPTIMIZER = tf.keras.optimizers.legacy.Adam(learning_rate=0.001) # use this if you have M1/M2 Mac, otherwise use the next line\n",
    "# OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = [tf.keras.metrics.F1Score('weighted')]\n",
    "EPOCHS = 20\n",
    "INPUT_SHAPE = (256, 219, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    train_f1_scores = history.history['f1_score']\n",
    "    val_f1_scores = history.history['val_f1_score']\n",
    "\n",
    "    epochs = range(1, len(train_f1_scores) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_f1_scores, 'bo-', label='Training F1 Score')\n",
    "    plt.plot(epochs, val_f1_scores, 'ro-', label='Validation F1 Score')\n",
    "    plt.title('Training and Validation F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Code Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(N_CLASSES, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    OPTIMIZER,\n",
    "    LOSS,\n",
    "    METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Your Results to Hugging Face Once Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: when naming your weights file\n",
    "# indicate the type of model\n",
    "# e.g.:\n",
    "## 'tuned_optimizer_model_weights.h5'\n",
    "## 'data_augmentation_model_weights.h5'\n",
    "WEIGHTS_TO_COMMIT = 'baseline_model_weights.h5'\n",
    "# save model weights\n",
    "model.save_weights(WEIGHTS_TO_COMMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model config\n",
    "CONFIG_TO_COMMIT  = 'model_config.json'\n",
    "model_config = model.get_config()\n",
    "model_config = json.dumps(model_config)\n",
    "with open(CONFIG_TO_COMMIT, 'w') as f:\n",
    "    f.write(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORMANCE_TO_COMMIT = 'baseline_model_performance.json'\n",
    "performance_dict = {\n",
    "    'train': {\n",
    "        'weighted_f1': history.history['f1_score'][-1]\n",
    "    },\n",
    "    'test': {\n",
    "        'weighted_f1': history.history['val_f1_score'][-1]\n",
    "    }\n",
    "}\n",
    "performance_dict = json.dumps(performance_dict)\n",
    "with open(PERFORMANCE_TO_COMMIT, 'w') as f:\n",
    "    f.write(performance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/fedorturchenko/.cache/huggingface/token\n",
      "Login successful\n",
      "Successfully logged out.\n"
     ]
    }
   ],
   "source": [
    "hgf.commit_to_hub(\n",
    "    repo=HGF_BASELINE_MODEL_REPO,\n",
    "    path_on_local=[WEIGHTS_TO_COMMIT, CONFIG_TO_COMMIT, PERFORMANCE_TO_COMMIT],\n",
    "    path_in_repo=[WEIGHTS_TO_COMMIT, CONFIG_TO_COMMIT, PERFORMANCE_TO_COMMIT],\n",
    "    # do not forget to change your commit message\n",
    "    # to add even more clarity\n",
    "    commit_message='upload baseline model weights, config, and scores'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files from local = clean up\n",
    "for path in [WEIGHTS_TO_COMMIT, CONFIG_TO_COMMIT, PERFORMANCE_TO_COMMIT]:\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
