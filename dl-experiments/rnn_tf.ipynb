{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np, pandas as pd, random\n",
    "from typing import Tuple, Literal, List\n",
    "from huggingface_hub import login, logout\n",
    "from bertopic import BERTopic\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class HGFresource:\n",
    "\n",
    "    def __init__(self, token):\n",
    "        # keep private\n",
    "        self.__token = token\n",
    "\n",
    "    def load_data(self, repo: str, sets: Literal['train', 'test', 'all'], sample_fraction: float) -> Tuple[np.ndarray]:\n",
    "        '''\n",
    "        Load dataset(s) from Hugging Face organization repository\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        repo : str\n",
    "            Path to the data repo on Hugging Face\n",
    "\n",
    "        sets : Literal['train', 'test', 'all']\n",
    "            Data sets to export \\n\n",
    "            Possible options: `['train', 'test', 'all']`\n",
    "\n",
    "        sample_fraction : float\n",
    "            Share of the dataset to return\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        `Tuple[np.ndarray]`\n",
    "            If `sets='train'` or `sets='test'`, returns corresponding images (converted to array) and labels, i.e. `images, labels` \\n\n",
    "            Otherwise, returns all train and test images (converted to array) and labels, i.e. `train_images, train_labels, test_images, test_labels`\n",
    "        '''\n",
    "\n",
    "        dataset = load_dataset(repo, token=self.__token)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tensorflow as tf\n",
    "\n",
    "HGF_TOKEN = os.environ['HUGGINGFACE_TOKEN']\n",
    "HGF_DATA_REPO = os.environ['HUGGINGFACE_DATASET_REPO']\n",
    "HGF_TOPIC_MODEL_REPO = os.environ['HUGGINGFACE_TOPIC_MODEL_REPO']\n",
    "\n",
    "hgf = HGFresource(token=HGF_TOKEN)\n",
    "dataset = hgf.load_data(repo=HGF_DATA_REPO, sets='all', sample_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train'].with_format('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(examples):\n",
    "    examples['image'] = [tf.cast(image.convert('RGB'), tf.float32) / 255.0 for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def preprocess_labels(example):\n",
    "    zeros = np.zeros(29)\n",
    "    np.put(zeros, example, 1)\n",
    "    zeros = tf.convert_to_tensor(zeros)\n",
    "    example = {'label': zeros}\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess_labels, input_columns=['label'])\n",
    "train_data = train_data.with_transform(preprocess_images, ['image'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to_tf_dataset(\n",
    "    columns=[\"image\"],\n",
    "    label_cols=[\"label\"],\n",
    "    batch_size=32,\n",
    "    # shuffle=True,\n",
    "    prefetch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = [tf.keras.metrics.F1Score('weighted')]\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(256, 219, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(29, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    OPTIMIZER,\n",
    "    LOSS,\n",
    "    METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=EPOCHS,\n",
    "    # validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, tensorflow as tf\n",
    "from data_loader.hgf_export import HGFresource\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "HGF_TOKEN = os.environ['HUGGINGFACE_TOKEN']\n",
    "HGF_DATA_REPO = os.environ['HUGGINGFACE_DATASET_REPO']\n",
    "HGF_TOPIC_MODEL_REPO = os.environ['HUGGINGFACE_TOPIC_MODEL_REPO']\n",
    "\n",
    "hgf = HGFresource(token=HGF_TOKEN)\n",
    "train_images, train_labels, test_images, test_labels = hgf.load_data(repo=HGF_DATA_REPO, sets='all', sample_fraction=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 11, 11, ..., 11, 21,  0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, a) -> None:\n",
    "        self.a = a\n",
    "    \n",
    "    def print_res(self):\n",
    "        def fun():\n",
    "            return self.a\n",
    "        print(fun())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = A(1)\n",
    "a.print_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = [tf.keras.metrics.F1Score('weighted')]\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "n_classes = np.unique(test_labels).shape[0]\n",
    "input_shape = test_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, n_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    OPTIMIZER,\n",
    "    LOSS,\n",
    "    METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data.batch(BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    # validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def history_plot(history):\n",
    "    train_loss = history.history['f1_score']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss, 'bo-')\n",
    "    plt.title('Training Weighted F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/grigoryturchenko/.cache/huggingface/token\n",
      "Login successful\n",
      "Successfully logged out.\n"
     ]
    }
   ],
   "source": [
    "topic_model = hgf.load_model(HGF_TOPIC_MODEL_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info = topic_model.get_topic_info()\n",
    "topics_info = topics_info[topics_info['Topic'] != -1]\n",
    "topics_info['Representation'] = topics_info['Representation'].apply(lambda x: [i for i in x if len(i) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = topics_info['Representation'].apply(lambda x: ' '.join(x)).to_numpy()\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenized_captions = [nltk.word_tokenize(caption) for caption in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(\n",
    "    set(\n",
    "        [word for sublist in topics_info['Representation'] for word in sublist]\n",
    "    )\n",
    ")\n",
    "tokenizer = Tokenizer(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(tokenized_captions)\n",
    "vocab = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(tokenized_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = topics_info['Representation'].apply(lambda x: len(x)).max()\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_seq = dict(zip(\n",
    "    [i for i in range(padded_sequences.shape[0])],\n",
    "    [i for i in padded_sequences]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_seq = np.array([topic_to_seq[i] for i in train_labels])\n",
    "test_labels_seq = np.array([topic_to_seq[i] for i in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder (CNN) input\n",
    "encoder_input = Input(shape=train_images[0].shape)\n",
    "\n",
    "# Define the CNN layers\n",
    "conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoder_input)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "flatten = Flatten()(pool3)\n",
    "hidden1 = Dense(128, activation='relu')(flatten)\n",
    "\n",
    "# Define the embedding layer to convert words to vectors\n",
    "embedding = Embedding(\n",
    "    input_dim=vocab_size + 1,\n",
    "    output_dim=128,\n",
    "    input_length=max_sequence_length,\n",
    ")(hidden1)\n",
    "\n",
    "# Define the LSTM layer for sequence generation\n",
    "lstm = LSTM(128, return_sequences=True)(embedding)\n",
    "\n",
    "# Define the output layer for generating sequences\n",
    "decoder_output = Dense(vocab_size, activation='softmax')(lstm)\n",
    "\n",
    "# Combine the encoder and decoder models to create the Seq2Seq model\n",
    "seq2seq_model = Model(encoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = ['f1_score']\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.compile(\n",
    "    OPTIMIZER,\n",
    "    LOSS,\n",
    "    METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = seq2seq_model.fit(\n",
    "    train_images,\n",
    "    train_labels_seq,\n",
    "    BATCH_SIZE,\n",
    "    EPOCHS,\n",
    "    validation_data=(test_images, test_labels_seq)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
